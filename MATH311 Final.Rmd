---
title: "MATH 311 Final Project"
author: "Ben Czekanski & Michael Czekanski"
date: "5/18/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(data.table)
library(doMC)
```

##Introduction
We are attempting to predict flight cancellations for flights leaving Boston using weather data from Boston, Atlanta, Detroit, and New York City. Our flight data contains 1.3 million flights leaving Boston from 2004-2012.


##Methods
While we are using all data from 2004 through 2009 as our training set, this data has over 700,000 entries and this it is computationally expensive to train on such a large data set. We believe that we should be able to effiectively train and get a good understanding of how our model is performing as long as we use at least 10,000 randomly selected entries from this training set. The training set we use to tune or model is actually only 2% of our total training data which equates to roughly 15,000 entries.
```{r}
set.seed(1)
training.set <- clean_flights %>%
  filter(DATE < as_date("2009-12-31")) %>%
  sample_frac(0.01)
```

#Basic Logistic Model

```{r}
training.model <- glm(factor(CANCELLED) ~ PRCP,
                      family = "binomial",
                      data = training.set)
summary(training.model)
```

We can begin predicting flight cancellations using the amount of precipitation in Boston on the day the flight occured. Although we see that precipitation has a statistically signifcant relationship with the log odds of a flight being cancelled, when we look at the predictions of flight cancellations its clear that precipitation performs no better than just guessing that no flights will be cancelled because so few flights are cancelled.

```{r}
test.set$pred.glm <- stats::predict.glm(training.model,
                                        test.set,
                                        type = "response")
fmsb::Kappa.test(table(test.set$pred.glm > 0.1, test.set$CANCELLED))
```

This model simply guesses that no flights will be cancelled because only 3% of flights in our dataset are cancelled. We can therefore say that this model is 97% accurate because it correctly predicts the results for 97% of the flights. We can also try this with more variables and regardless of statistical significance the predictions don't change.

```{r}
training.model <- glm(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE + 
                        TMAX + TMIN + WT01 + WT03 + PRCP + SNOW + WESD,
                      family = "binomial",
                      data = training.set)
summary(training.model)
```

Again we can show that this model predicts no cancelled flights correctly.

```{r}
test.set$pred.glm <- stats::predict.glm(training.model,
                                        test.set,
                                        type = "response")
table(test.set$pred.glm > 0.5, test.set$CANCELLED)
fmsb::Kappa.test(table(test.set$pred.glm > 0.16, test.set$CANCELLED))$Result$estimate
```


The accuracy of both of these models is 97% which is would be impressive if we could not also achieve this level of accuracy by guessing that no flights will be cancelled at all. A better metric we can use to evaluate models is Cohen's Kappa which measures the amount of information added by the model by quantifying how much of the gap between random guessing and perfect prediction we have made up with our model. With such a strong trend in our data (97% of flights being cancelled) this metric will punish the model strongly for incorrectly predicting against the trend and heavily reward it for correctly predicting against the trend which emphasizes the model's ability to accurately predict against the trend. We can improve this prediction by using the Boosted algorithm LogitBoost (caret package) in which we bootstrap our model through resampled data. In each iteration the data is sampled, with harder to predict observations being weighted heavier, a logistic model is fit to the data and in the end the models are averaged weighted by their performance on the training data. This model also uses decision stumps as opposed to a linear model, where a standard logistic model creates a linear relationship between log odds and inputs LogitBoost creates a threshold for each predictor variable and uses this as a binary cutoff for predicion of our binary predicted variable. A simple use of this algorithm is shown below.

```{r}
print(Sys.time())
alex <- train(factor(CANCELLED) ~ PRCP,
              method = "LogitBoost",
              metric = "Kappa",
              data = training.set)

print(Sys.time())
alex$results
alex$finalModel
```

Here, with only one feature the model uses this one feature in each iteration with threshold 4.29 with a sign of -1. Therefore if precipitation is below 4.29 on the day that the flight occurred each iteration of the model will "vote" that the flight is not cancelled.

Now to calculate Cohen's Kappa we can sue the function Kappa.test our of the "fmsb" package on a confusion matrix of our predictions vs. the truth about each cancellation.

Confusion Matrix:
```{r}
test.set$pred.ml <- predict(alex,
                         test.set)
table(test.set$CANCELLED, test.set$pred.ml)
```

```{r}
test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate
test.Kappa.val.ml
```
Here test Kappa is again zero for this basic model but we believe it will improve greatly with more predictor variables and use of this algorithm.

A more complex model:

```{r}
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE +
                TMAX + TMIN + WT01 + WT03 + SNOW + PRCP + AWND + WESD +
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = training.set)

alex$results
alex$finalModel

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate
training.Kappa.val.ml
test.Kappa.val.ml
```

One large issue that we ran into is demonstrated here, the model repeatedly picks up on distance as an important variable and repeatedly makes thresholds around 860 but the sign associated with each is routinely changing. We can interpret this as distance being a good predictor in each iteration but is not very good as a predictor overall as the signs for identical thresholds are inconsistent. This tells us that we should remove distance from the model because while training the algorithm is spending a lot of iterations focusing on distance with little to show for it.

```{r}
#REMOVED DISTANCE
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) +
                TMAX + TMIN + WT01 + WT03 + SNOW + PRCP + AWND + WESD +
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = training.set)

alex$results
alex$finalModel

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

training.Kappa.val.ml
test.Kappa.val.ml
```

TMIN again switches signs with a threshold of roughly 70.

```{r}
#REMOVED TMIN and ADDED TMIN.l32 and TMAX.l32
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) +
                TMAX + WT01 + WT03 + SNOW + PRCP + AWND + WESD + PRCP*PRCP.TMIN.l32 + PRCP*PRCP.TMAX.l32 + 
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = training.set)

alex$results
alex$finalModel

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

training.Kappa.val.ml
test.Kappa.val.ml
```

```{r}
#REMOVED PRCP
#PRCP.TMIN.l32
#TMAX.nyc
#TMAX
#WT03
#MONTH
#PRCP.nyc
#TMIN_lag1.atl
#WT01 & WT01.nyc & PRCP
alex <- train(factor(CANCELLED) ~ factor(DAY_OF_WEEK) +
                SNOW + AWND + WESD + PRCP.TMAX.l32 + 
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMIN.nyc + WT03.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = training.set)
alex$results
alex$finalModel

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

training.Kappa.val.ml
test.Kappa.val.ml
```

Often times the LogitBoost algorithm cycles through these variables and we would end up removing variables such as precipitation or snow in Boston which should definitely be useful in predicting flight cancellations. To make sense of these cycles we would have to look at interactions between multiple variables. For example we would need to look at the amount of precipitation when the temperature was below freezing because we expect that this precipitation would better predict a cancellation than rain would. To account for these interactions between variables we transitioned to using random forests. Where LogitBoost makes one threshold per iteration a random forest makes multiple thresholds and does not re-weight observations for resampling between iterations. These multiple thresholds will account for the interactions between variables although resampling will remove emphasis on correctly predicting observations that are hard to predict.

```{r}
Sys.time()
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE +
                TMAX + TMIN + WT01 + WT03 + SNOW + PRCP + AWND + WESD +
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "rf",
              metric = "Kappa",
              data = sample_frac(training.set, 1))
Sys.time()
alex$results
alex$finalModel

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate
training.Kappa.val.ml
test.Kappa.val.ml
```

These random forests are an improvement over LogitBoost but are much more computationally expensive and do not yield much better results and also has a tendency to overfit.

#Revisiting Previous Models

One way that we can attempt to get a better idea of whether or not a flight will be cancelled from these models is re-evaluating our cutoff for predicted probabilities. In our original logistic models we used a cutoff of 0.5, meaning that if P(cancellation) > 0.5 we classify the flight as cancelled and if P(cancellation < 0.5) we classify the flight as not cancelled. If our model has spikes in predicted probability for flights that are cancelled we can capture this through a new cutoff. This is computationally inexpensive and intuitive that our model would have a hard time predicting P(cancellation) > 0.5 because so few flights are actually cancelled. We investigate a better cutoff in the code below:

```{r}

fit.glm.once <- function(){
  training.model <- glm( factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + 
                           TMAX + TMIN + WT01 + WT03 + SNOW + WESD +
                           TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                           TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det + 
                           TMAX.nyc + TMIN.nyc + WT03.nyc + WESD.nyc +
                           TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                           TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                           TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                           PRCP_lag1.det,
                         family = "binomial",
                         data = clean_flights)
  summary(training.model)
  
  training.set$pred.glm <- stats::predict.glm(training.model,
                                              training.set,
                                              type = "response")
  train.quant.97 <- quantile(training.set$pred.glm, 0.97)
  
  test.set$pred.glm <- stats::predict.glm(training.model,
                                          test.set,
                                          type = "response")
  test.quant.97 <- quantile(test.set$pred.glm, 0.97)
  
  results <- data.frame()
  for(i in seq(from = 0.05, to = 0.95, by = 0.01)){
    if (sum(training.set$pred.glm > i) == 0){
      train.results <- data.frame(Cutoff = i, Fraction = frac, Kappa = 0, Set = "Train", Quanitle.97 = train.quant.97)
    } else{
      glm.training.Kappa.val <- (fmsb::Kappa.test(table(training.set$pred.glm > i, training.set$CANCELLED)))$Result$estimate
      train.results <- data.frame(Cutoff = i, Fraction = frac, Kappa = glm.training.Kappa.val, Set = "Train", Quanitle.97 = train.quant.97)
    }
    
    if(sum(test.set$pred.glm > i) == 0){
      test.results <- data.frame(Cutoff = i, Fraction = frac, Kappa = 0, Set = "Train", Quanitle.97 = test.quant.97)
    } else{
      glm.test.Kappa.val <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.glm > i)))$Result$estimate
      test.results <- data.frame(Cutoff = i, Fraction = frac, Kappa = glm.training.Kappa.val, Set = "Test",  Quanitle.97 = test.quant.97)
    }
    
    results <- rbind(train.results, test.results, results)
  }
  return(results)
}

glm.cutoffs <- fit.glm.once()

glm.cutoffs %>%
  ggplot(aes(x = Cutoff, y = Kappa)) +
  geom_point() +
  labs(title = "Kappa vs. Cutoff") + 
  theme_bw() +
  scale_color_colorblind()
```

As shown in this graph the cutoff we use can have a large impact on the Kappa we achieve. To the optimal cutoff we began by checking to see if the cutoff is the 0.97 quantile of the predicted probabilities which we intuit from the fact that 97% of flights are cancelled so we hope that these are the 97% of the lowest predicted probabilities of cancellation. To highlight this on the graph we can highlight whether the given cutoff is above or below the 0.97 quantile of the predicted probabilities.

```{r}
glm.cutoffs %>%
  mutate(above.quant = Cutoff > Quanitle.97) %>%
  ggplot(aes(x = Cutoff, y = Kappa)) +
  geom_point(aes(color = above.quant)) +
  scale_color_colorblind() + 
  facet_grid(~Set)
```

Here the Kappa values for both the test and training sets are shown with various cutoffs. We can see that the 0.97 quantile of the predicted proabbilities seems to be the best cutoff in that it achieves the highest Kappa. 





```{r}
flights.bos <- fread("FlightsData0212BOS.csv")
weather.data <- fread("BOSNYCweather0212.csv")


weather <- weather.data %>%
  mutate(DATE = as_date(DATE)) %>%
  mutate(SNOW = as.numeric(SNOW),
         PRCP = as.numeric(PRCP),
         SNWD = as.numeric(SNWD),
         TMIN = as.numeric(TMIN),
         TMAX = as.numeric(TMAX),
         TAVG = as.numeric(TAVG))

weather.nyc <- weather %>%
  filter(NAME == "NY CITY CENTRAL PARK, NY US")

weather.bos <- weather %>%
  filter(NAME == "BOSTON, MA US")
```

##Feature Selection. 
We speculate that the weather in Boston is different from the weather in NYC and that this difference will help us to predict cancellations of flights in Boston. We must test to see if the weather is different in these two cities.
```{r}
hist(weather.bos$TAVG - weather.nyc$TAVG)
```

This is roughly normally distributed so we can perform a t-test.

```{r}
t.test(weather.bos$TAVG - weather.nyc$TAVG)
```

THE AVERAGE TEMPERATURE IS NOT THE SAME.