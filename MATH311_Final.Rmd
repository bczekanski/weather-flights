---
title: "MATH 311 Final Project"
author: "Ben Czekanski & Michael Czekanski"
date: "5/18/2018"
output:
  html_document: default
  pdf_document: default
---

TO DO:

Where does the data come from? BEN

Kappa values for training and test for all models

Should the training kappas be the small training set?

Explanation of biased estimator MICHAEL
 
Clarify Kappa explanation MICHAEL

Add cutoff selection for LogitBoost 

Change cutoff for RF?? MICHAEL

Run models on secret test set

Conf matrices consistently test or train

RF conf mat

```{r setup, include = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = TRUE)
library(tidyverse)
library(lubridate)
library(data.table)
library(doMC)
library(caret)
library(ggthemes)
library(stargazer)
library(knitr)

set.seed(1)
# Update this 
clean.flights <- fread("~/Desktop/FlightData2.csv")
# write.csv(sample_frac(clean.flights, 0.2), file = "~/Desktop/FlightData2.csv")

training.set <- clean.flights %>%
  filter(DATE < as_date("2010-01-01")) 

small.training.set <- training.set%>%
  sample_frac(0.01)
  #select(-c(STATION, NAME, DATE, YEAR, DAY_OF_MONTH, FL_DATE, TAIL_NUM, FL_NUM, ORIGIN_AIRPORT_ID, ORIGIN_STATE_ABR, DEST_AIRPORT_ID, DEST_STATE_ABR, DEP_TIME, 217:231, 233:238, 240:246))


test.set <- clean.flights %>%
  filter(DATE >= as_date("2010-01-01") & DATE < as_date("2012-01-01"),
         DEST %in% unique(training.set$DEST),
         UNIQUE_CARRIER %in% unique(training.set$UNIQUE_CARRIER))

#DO NOT TEST ON
#TO BE USED TO EVALUATE FINAL MODEL
secret.test <- clean.flights %>%
  filter(DATE >= as_date("2012-01-01"),
         DEST %in% unique(training.set$DEST),
         UNIQUE_CARRIER %in% unique(training.set$UNIQUE_CARRIER))

```

## Introduction

At 10:35 am on December 17, 1903, Orville Wright was the first human to experience mechanical flight. As Orville reached this magical high that millions of people have experienced since, Wilbur was the first human to experience the terrible low that is a flight cancellation. The brothers flipped a coin to choose who got to fly first that day, but in our paper we try to do better than chance in predicting flight cancellations. We use daily atmospheric data from Boston, Atlanta, Detroit, and New York City to predict \(\textit{whether}\) (weather, geddit?) flights leaving Boston will be cancelled.

## Data

## Where does the data come from??

Our flight data contains `r nrow(clean.flights)` flights leaving Boston's Logan Airport from `r min(clean.flights$DATE)` to `r max(clean.flights$DATE)`. There are `r sum(clean.flights$CANCELLED)` total cancellations. The most cancellations on a single day is `r clean.flights %>% group_by(DATE) %>% summarize(cancellations = sum(CANCELLED)) %>% arrange(desc(cancellations)) %>% .[1,2] %>% as.character()` cancellations on `r clean.flights %>% group_by(DATE) %>% summarize(cancellations = sum(CANCELLED)) %>% arrange(desc(cancellations)) %>% .[1,1] %>% as.character()`. For each day in this period we have the weather in Boston, New York City, Detroit, and Atlanta. These weather in these major hubs gives us a sense if there are storms around the country that might affect flights in Boston even if there is nice weather in Massachusetts. Additionally, we incorporate lagged weather variables to account for storms that might move from one of these cities into Boston and affect flights in this way. 

The variables that we have are:

`r paste(names(clean.flights), sep = ", ")`

```{r}
clean.flights %>%
  group_by(DATE) %>%
  summarize(cancellations = sum(CANCELLED)) %>%
  ggplot2::ggplot(aes(x = as_date(DATE), y = cancellations)) +
  geom_col() +
  labs(x = "Date", y = "Cancellations", title = "Logan Airport Flight Cancellations by Day", subtitle = "September 2003 to December 2012") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_x_date(date_breaks = "1 year") +
  theme_bw()
# CAPTION
```

We partition our data into three subsets. The first, the training set, contains data from 2003-2009. The second, what we call the test set and what should technically be called the validation set, contains flights from 2010-2011. We call the third subset the "secret test set" (this should be called the test set). This contains the flights in 2012.

We use the first two subsets to train our model and prevent overfitting. We first train our model on the training set, and then look at how it performs in the test set. If we see that the model performs substantially better on the training set than on the test set, we will know that our model is overfitting and picking up trends in the training set that do not exist in the test set. We break these sets up by date because this allows us to "get into a time machine" to the end of the training set and imagine that we have all the data up to that date and want to predict flights after that date. An important assumption that we make in order to break the data up by date is that there is no structural change in how or why flights are cancelled in Boston that happens near the date of the split. After selecting a model that we think fits the data using the training and test sets, we peek at the "secret test set". Our performance on the "secret test set" gives us an idea of how we really do out of sample, and what our model's performance would be if we wanted to predict cancellations tomorrow.

While we are using all the flights we have from 2003 through 2009 as our training set, this data has over 700,000 entries and thus it is computationally expensive to train on such a large data set. We believe that we should be able to effectively train a model and understand its performing as long as we use at least 10,000 randomly selected entries from this training set. The training set we use to tune our model is actually only 2% of our total training data, which equates to roughly 15,000 entries.

```{r}
set.seed(1)
training.set <- clean.flights %>%
  filter(DATE < as_date("2009-12-31")) %>%
  sample_frac(0.01)
```

#Basic Logistic Model

We began creating our model using basic logistic regression to classify a given flight as either cancelled (1) or not cancelled (0) using the amount of precipitation on that day (in inches).

```{r, results = "asis"}
training.model <- glm(factor(CANCELLED) ~ PRCP,
                      family = "binomial",
                      data = training.set)
# summary(training.model)
stargazer(training.model,
          type = "html",
          dep.var.labels = "Cancelled",
          title = "Basic Logistic Regression",
          single.row = TRUE)
```

We started by using the amount of precipitation in Boston on the day the flight occured as our predictor variable. Although we see that precipitation has a statistically signifcant relationship with the log odds of a flight being cancelled, when we look at the predictions of flight cancellations its clear that precipitation performs no better than just guessing that no flights will be cancelled because so few flights are cancelled.

```{r}
test.set$pred.glm <- stats::predict.glm(training.model,
                                        test.set,
                                        type = "response")

conf.mat <- rbind(table(test.set$pred.glm > 0.5, test.set$CANCELLED), c(0, 0))

rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")

kable(conf.mat, 
      caption = "Basic Logistic Regression Test Set Confusion Matrix", 
      align  = "l", 
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))


# kable(fmsb::Kappa.test(conf.mat)$Result$estimate,
#       col.names = "Test Kappa",
#       caption = "Basic Logistic Regression Kappa Value")
```

This model simply guesses that no flights will be cancelled because only 3% of flights in our dataset are cancelled. We can therefore say that this model is 97% accurate because it correctly predicts the results for 97% of the flights. However this model does not add to our understanding of flight cancellations. 

## Taking a Step Back: Cohen's Kappa

Model Accuracy of 97% would be impressive if we could not also achieve this level of accuracy by guessing that no flights will be cancelled at all. A better metric we can use to evaluate models is Cohen's Kappa, which measures the amount of information added by the model. Cohen's Kappa quantifies how much of the gap between random guessing and perfect prediction we have made up with our model and can be calculated using the equation below.

$$
K = \frac{P_O - P_E}{1 - P_E}\\
P_O = P(\text{Agreement Observed}) \\
P_E = P(\text{Agreement Expected Due to Chance})
$$

\(P_E\) is the probability that the model and the truth agree by random chance. It is calculated as 

$$
\begin{align*}
P_O &= P(\text{model and truth agree}) \\
&= P(\text{both agree flight cancelled}) + P(\text{both agree flight not cancelled}) \\
&= P(\text{model} = 1)P(\text{truth} = 1) + P(\text{model} = 0)P(\text{truth} = 0)
\end{align*}
$$

We can understand this as the probability that the model and the true values if their classifications had both been randomly assigned to observations. We would expect them to agree with probability \(P_E\), but these classifications can be distributed in many different permutations and some are better than others. We need to compare our results to this baseline of randomly permuted classifications so we look at the probability that our model and the truth agreed based on the observed matches. We use \(P_O\) which is the probability that the model and the truth agree as observed from our predictions. It is calculated as 

$$
P_E = P(\text{prediction is correct}) \\
= \text{mean}(\text{predicted value} == \text{truevalue})\\
\text{THIS SHOULD BE A FRACTION, NOT R CODE}
$$

Cohen's Kappa tells us how much better we are doing than randomly permuting our classifications and thus we want this number to be higher. It is bounded above by 1 which would mean we are predicting perfectly. A Kappa of 0 is equivalent to random permutations. Although Kappa is not bounded below by 0, values below zero mean that we are performing worse than random guessing which is very bad.

With such a strong trend in our data (97% of flights being cancelled) this metric will punish the model strongly for incorrectly predicting against the trend and heavily reward it for correctly predicting against the trend, emphasizing the model's ability to accurately predict against the trend. 

## Multivariable Logistic Regression Model

We then try another logistic regression, this time with more variables.

```{r, results = "asis"}
training.model <- glm(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE + 
                        TMAX + TMIN + PRCP + SNOW + WESD + 
                        TMAX.atl + TMIN.atl + PRCP.atl + WESD.atl +
                        TMAX.nyc + TMIN.nyc + PRCP.nyc + WESD.nyc + 
                        TMAX.det + TMIN.det + PRCP.det + WESD.det, 
                      family = "binomial",
                      data = small.training.set)
stargazer(training.model,
          type = "html",
          dep.var.labels = "Cancelled",
          title = "Multivariable Logistic Regression",
          single.row = TRUE)
```


Now to calculate Cohen's Kappa we can use the `fmsb::Kappa.test()` function on a confusion matrix of our predictions vs. the truth about each cancellation.

```{r}
training.set$pred.glm <- stats::predict.glm(training.model,
                                        training.set,
                                        type = "response")

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.glm > 0.5, training.set$CANCELLED)))$Result$estimate

test.set$pred.glm <- stats::predict.glm(training.model,
                                        test.set,
                                        type = "response")
conf.mat <- table(list(test.set$pred.glm > 0.5, test.set$CANCELLED))
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")

kable(conf.mat, 
      caption = "Multivariable Logistic Regression Test Set Confusion Matrix", 
      align  = "l", 
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))

kable(data.frame(training.Kappa.val.ml, fmsb::Kappa.test(conf.mat)$Result$estimate),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Multivariable Logistic Regression Kappa Values")
```

## Single Variable LogitBoost Model

We can improve this prediction by using the Boosted algorithm LogitBoost from the `caret` package, which allows us to bootstrap our model through resampled data. In each iteration, the data is sampled, with harder to predict observations given larger sample weights. Then a logistic model is fit to the bootstrapped data. Once models have been fit to every bootstrapped sample, they are all averaged, weighted according to their performance on the training data. 


### CLARIFY THIS MICHAEL
Another aspect of LogitBoost is its use of decision stumps. This differs from a linear model, where a standard logistic model creates a linear relationship between log odds and inputs LogitBoost creates a threshold for each predictor variable and uses this as a binary cutoff for predicion of our binary predicted variable. A simple use of this algorithm is shown below.

```{r, echo = TRUE}
# print(Sys.time())
alex <- train(factor(CANCELLED) ~ PRCP,
              method = "LogitBoost",
              metric = "Kappa",
              data = small.training.set)
```

```{r}
# print(Sys.time())
kable(alex$results, caption = "Single Variable LogitBoost Results")
kable(alex$finalModel$Stump, caption = "Single Variable LogitBoost Decision Stumps")
```

Here, with only one feature, the model uses this feature in each iteration with threshold 4.29 with a sign of -1. This means that if precipitation is below 4.29 on the day that the flight occurred each iteration of the model will "vote" that the flight is not cancelled.


```{r}
training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                         test.set)
# kable(table(test.set$CANCELLED, test.set$pred.ml))

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Single Variable LogitBoost Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation")
      )
```

```{r}
training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate
# test.Kappa.val.ml

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Single Variable LogitBoost Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation")
      )

kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Single Variable LogitBoost Regression Kappa Values")
```

Here test Kappa is again zero for this basic model but we believe it will improve greatly with more predictor variables.

## Multivariable LogitBoost Model

A more complex model:

```{r, echo = TRUE}
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE +
                TMAX + TMIN + WT01 + WT03 + SNOW + PRCP + AWND + WESD +
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = small.training.set)

```

```{r}
kable(alex$results, caption = "Multivariable LogitBoost Results")

feature.df <- data.frame(alex$finalModel$Stump) %>% left_join(data.frame(feature = 1:56, Name = alex$finalModel$xNames), by = "feature")

kable(feature.df, caption = "Multivariable LogitBoost Decision Stumps")

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Multivariable LogitBoost Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))

kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Multivariable LogitBoost Kappa Values")
```

One large issue that we ran into is demonstrated here; the model repeatedly picks up on "DISTANCE" as an important variable. LogitBoost repeatedly makes thresholds around 860 but the sign associated with each is flip-flops. We can interpret this as "DISTANCE" being a good predictor in each iteration but not a very good predictor overall, as the signs for identical thresholds are inconsistent. This suggests that we should perhaps remove "DISTANCE" from the model because the algorithm is spending a lot of iterations focusing on "DISTANCE" with little to nothing show for it and could probably improve by focusing on other variables.


```{r, echo = TRUE}
#REMOVED DISTANCE
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) +
                TMAX + TMIN + WT01 + WT03 + SNOW + PRCP + AWND + WESD +
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = small.training.set)
```

```{r}
kable(alex$results, caption = "Multivariable LogitBoost Results")

feature.df <- data.frame(alex$finalModel$Stump) %>% left_join(data.frame(feature = 1:55, Name = alex$finalModel$xNames), by = "feature")

kable(feature.df, caption = "Multivariable LogitBoost Decision Stumps")

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Multivariable LogitBoost Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))

kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Multivariable LogitBoost Kappa Values")
```

Here AWND.atl changes sign at almost identical thresholds so it is next removed along with TMIN. Here we also added interaction terms between the amount of precipitation and an indicator for whether or not TMIN or TMAX is less than 32 (=1 if TMAX|TMIN < 32). 

```{r, echo = TRUE}
#REMOVED TMIN and ADDED TMIN.l32 and TMAX.l32
alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) +
                TMAX + WT01 + WT03 + SNOW + PRCP + AWND + WESD + PRCP*PRCP.TMIN.l32 + PRCP*PRCP.TMAX.l32 + 
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = small.training.set)
```

```{r}
kable(alex$results, caption = "Multivariable LogitBoost Results")

feature.df <- data.frame(alex$finalModel$Stump) %>% left_join(data.frame(feature = 1:57, Name = alex$finalModel$xNames), by = "feature")

kable(feature.df, caption = "Multivariable LogitBoost Decision Stumps")

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Multivariable LogitBoost Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))


kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Multivariable LogitBoost Kappa Values")
```

For similar reasons we now remove REMOVED PRCP, PRCP.TMIN.l32, TMAX.nyc, TMAX, WT03, MONTH, PRCP.nyc, TMIN_lag1.atl, WT01, WT01.nyc, PRCP.

```{r, echo = TRUE}
#REMOVED PRCP
#PRCP.TMIN.l32
#TMAX.nyc
#TMAX
#WT03
#MONTH
#PRCP.nyc
#TMIN_lag1.atl
#WT01 & WT01.nyc & PRCP
alex <- train(factor(CANCELLED) ~ factor(DAY_OF_WEEK) +
                SNOW + AWND + WESD + PRCP.TMAX.l32 + 
                TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
                TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
                TMIN.nyc + WT03.nyc + WESD.nyc +
                TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
                TMAX_lag1.atl + PRCP_lag1.atl +
                TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
                PRCP_lag1.det,
              method = "LogitBoost",
              metric = "Kappa",
              data = small.training.set)
```

```{r}
kable(alex$results, caption = "Multivariable LogitBoost Results")

feature.df <- data.frame(alex$finalModel$Stump) %>% left_join(data.frame(feature = 1:36, Name = alex$finalModel$xNames), by = "feature")

kable(feature.df, caption = "Multivariable LogitBoost Decision Stumps")

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Multivariable LogitBoost Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))


kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Multivariable LogitBoost Kappa Values")
```

At this point this algorithm continues to cycle thorough variables and we will end up removing ones that are rather intuitive for the prediction of flight cancellations such as snow and precipitation. Additionally, this technique does not seem to help, as each time we remove variables our Kappa decreases. To make sense of these cycles we would have to look at interactions between multiple variables. For example, we would need to look at the amount of precipitation when the temperature was below freezing because we expect that this precipitation would better predict a cancellation than rain would. 

## Introducing Random Forests

To account for these interactions between variables we transitioned to using random forests. Where LogitBoost makes one threshold per iteration, a random forest makes multiple thresholds that each factor into its predictions and does not re-weight observations for resampling between iterations. These multiple thresholds will account for the interactions between variables although resampling will remove emphasis on correctly predicting observations that are hard to predict.

```{r, echo = TRUE}

# alex <- train(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE +
#                 TMAX + TMIN + WT01 + WT03 + SNOW + PRCP + AWND + WESD +
#                 TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
#                 TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
#                 TMAX.nyc + TMIN.nyc + WT01.nyc + WT03.nyc + PRCP.nyc + WESD.nyc +
#                 TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
#                 TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
#                 TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
#                 PRCP_lag1.det,
#               method = "rf",
#               metric = "Kappa",
#               data = sample_frac(small.training.set, 1))
```

```{r}
load("rf.model")

kable(alex$results, caption = "Random Forest Results")


# alex$finalModel
# Add this back

training.set$pred.ml <- predict(alex,
                                training.set)

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate

conf.mat <- table(test.set$pred.ml, test.set$CANCELLED)
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")
# FIGURE OUT CONFUSION MATRIX
kable(conf.mat, 
      caption = "Random Forest Test Set Confusion Matrix", 
      align  = "l",
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))

kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "Random Forest Kappa Values")
```

These random forests are an improvement over LogitBoost but are much more computationally expensive and do not yield much better results. This model also has a tendency to overfit, as shown above.

#Revisiting Previous Models

One way that we can attempt to get a better idea of whether or not a flight will be cancelled from these models is re-evaluating our cutoff for predicted probabilities. In our original logistic models we used a cutoff of 0.5, meaning that if P(cancellation) > 0.5 we classify the flight as cancelled and if P(cancellation < 0.5) we classify the flight as not cancelled. If our model has spikes in predicted probability for flights that are cancelled we can capture this through a new cutoff. This is computationally inexpensive and intuitive that our model would have a hard time predicting P(cancellation) > 0.5 because so few flights are actually cancelled. We try different cutoffs and look at the Kappa values that these new cutoffs give us below.

```{r, fig.cap = "This figure shows the Kappa value at different cutoffs for our Multivariable Logistic Regression model. We see that the cutoffs that give us the highest Kappa values are not at 0.5, but between 0.1 and 0.2."}
# 
# fit.glm.once <- function(){
#   p.cancel <- mean(small.training.set$CANCELLED)
#   training.model <- glm( factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) +
#                            TMAX + TMIN + WT01 + WT03 + SNOW + WESD +
#                            TMAX.atl + TMIN.atl + WT01.atl + WT03.atl + PRCP.atl + AWND.atl + WESD.atl +
#                            TMAX.det + TMIN.det + WT01.det + WT03.det + PRCP.det + WESD.det +
#                            TMAX.nyc + TMIN.nyc + WT03.nyc + WESD.nyc +
#                            TMAX_lag1 + TMIN_lag1 + SNOW_lag1 + PRCP_lag1 +
#                            TMAX_lag1.atl + TMIN_lag1.atl + PRCP_lag1.atl +
#                            TMAX_lag1.nyc + TMIN_lag1.nyc + PRCP_lag1.nyc +
#                            PRCP_lag1.det,
#                          family = "binomial",
#                          data = training.set)
#   summary(training.model)
# 
#   training.set$pred<- predict.glm(training.model,
#                               training.set,
#                               type = "response")
#   train.quant.97 <- quantile(training.set$pred, 0.97)
#   train.quant.pcancel <- quantile(training.set$pred, 1-mean(training.set$CANCELLED))
#   
#   test.set$pred <- predict.glm(training.model,
#                               test.set,
#                               type = "response")
#   test.quant.97 <- quantile(test.set$pred, 0.97)
#   test.quant.pcancel <- quantile(test.set$pred, 1-mean(test.set$CANCELLED))
#   
#   results <- data.frame()
#   for(i in seq(from = 0.05, to = 0.95, by = 0.01)){
#     print(i)
#     if (sum(training.set$pred > i) == 0 | sum(training.set$pred <= i) == 0 ){
#       train.results <- data.frame(Cutoff = i,
#                                   Kappa = 0,
#                                   Set = "Train",
#                                   Iteration = iter,
#                                   Quantile.97 = train.quant.97,
#                                   PCancel = p.cancel,
#                                   QuantilePCancel = train.quant.pcancel)
#     } else{
#       glm.training.Kappa.val <- (fmsb::Kappa.test(table(training.set$pred > i, training.set$CANCELLED)))$Result$estimate
#       train.results <- data.frame(Cutoff = i,
#                                   Kappa = glm.training.Kappa.val,
#                                   Set = "Train",
#                                   Iteration = iter,
#                                   Quantile.97 = train.quant.97,
#                                   PCancel = p.cancel,
#                                   QuantilePCancel = train.quant.pcancel)
#     }
#     
#     if(sum(test.set$pred > i) == 0 | sum(test.set$pred <= i) == 0) {
#       test.results <- data.frame(Cutoff = i,
#                                  Kappa = 0,
#                                  Set = "Train",
#                                  Iteration = iter,
#                                  Quantile.97 = test.quant.97,
#                                  PCancel = p.cancel,
#                                  QuantilePCancel = test.quant.pcancel)
#     } else{
#       glm.test.Kappa.val <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred > i)))$Result$estimate
#       test.results <- data.frame(Cutoff = i,
#                                  Kappa = glm.test.Kappa.val,
#                                  Set = "Test",
#                                  Iteration = iter,
#                                  Quantile.97 = test.quant.97,
#                                  PCancel = mean(test.set$CANCELLED),
#                                  QuantilePCancel = test.quant.pcancel)
#     }
#     
#     results <- rbind(train.results, test.results, results)
#   }
#   return(results)
# }
# 
# glm.cutoffs <- fit.glm.once()

glm.cutoffs <- fread("CutoffsLog.csv")

glm.cutoffs %>%
  ggplot(aes(x = Cutoff, y = Kappa, color = Set)) +
  geom_point() +
  labs(title = "Multivariable Binomial Regression Kappa vs. Cutoff") +
  theme_bw() +
  scale_color_colorblind()
```

This graph shows how the cutoff we use can have a large impact on the Kappa we achieve. To find the optimal cutoff we began by checking to see if the cutoff is above the 0.97 quantile of the predicted probabilities. If 97% of flights are cancelled we hope that these are the 97% of the lowest predicted probabilities of cancellation. To highlight this on the graph we can look at whether the given cutoff is above or below the 0.97 quantile of the predicted probabilities.

```{r, fig.cap = "Here the Kappa values for both the test and training sets are shown with various cutoffs. We can see that where we go from above the 0.97 quantile  to below the 0.97 quantile of predicted probabilities seems to be the cutoff in that it achieves the highest Kappa."}

glm.cutoffs %>%
  mutate(above.quant = Cutoff > Quantile.97) %>%
  ggplot(aes(x = Cutoff, y = Kappa)) +
  geom_point(aes(color = above.quant)) +
  scale_color_colorblind(name = "0.97 Quantile", 
                         labels = c("Below", "Above")) +
  facet_grid(~factor(Set, levels = c("Train", "Test"))) +
  theme_bw() +
  labs(title = "Binomial Multivariable Regression Kappa vs Cutoff, with 0.97 Quantile",
       subtitle = "By Set")
```

```{r}

rf.cutoffs <- fread("CutoffsRF.csv")
cutoffs.sim <- rf.cutoffs

num.iter <- cutoffs.sim$Iteration %>%
  unique() %>%
  length()
diffs <- NULL
for(i in 1:num.iter){
  # print(i)
  temp.df <-  cutoffs.sim %>%
    filter(Iteration == i,
           Set == "Test")
  max.kappa <- max(temp.df$Kappa)
  max.cutoff <- temp.df %>%
    filter(Kappa == max.kappa) %>%
    .$Cutoff %>%
    .[1]
  my.cutoff <- temp.df %>%
    filter(abs(Cutoff - Quantile.97) < 0.005) %>%
    .$Cutoff
  diffs[i] <- max.cutoff - my.cutoff
}
hist(diffs)
t.test(diffs)

diffs.boot <- NULL
for(i in 1:10000){
  samp <- sample(diffs, replace = TRUE)
  diffs.boot[i] <- mean(samp)
}
mean(diffs.boot)
hist(diffs.boot)
mean(diffs.boot > mean(diffs))

bias <- mean(diffs)
unbiased.diffs <- NULL
for(i in 1:num.iter){
  # print(i)
  temp.df <-  cutoffs.sim %>%
    filter(Iteration == i,
           Set == "Test")
  max.kappa <- max(temp.df$Kappa)
  max.cutoff <- temp.df %>%
    filter(Kappa == max.kappa) %>%
    .$Cutoff %>%
    .[1]
  my.cutoff <- temp.df %>%
    filter(abs(Cutoff - Quantile.97) < 0.005) %>%
    .$Cutoff
  unbiased.diffs[i] <- max.cutoff - my.cutoff - bias
}
mean(unbiased.diffs)
hist(unbiased.diffs)
t.test(unbiased.diffs)
bias


cutoffs.sim %>%
  filter(Set == "Test") %>%
  ggplot(aes(x = Cutoff, y = Kappa)) +
  geom_point(alpha = 0.2) +
  theme_bw()

```

## A New Multivariable Logistic Regression Model

We can make an improved Multivariable Logistic Regression model by changing the cutoff to .....?0.1?????. This model is shown below.

```{r, results = "asis"}
training.model <- glm(factor(CANCELLED) ~ factor(MONTH) + factor(DAY_OF_WEEK) + DISTANCE + 
                        TMAX + TMIN + PRCP + SNOW + WESD + 
                        TMAX.atl + TMIN.atl + PRCP.atl + WESD.atl +
                        TMAX.nyc + TMIN.nyc + PRCP.nyc + WESD.nyc + 
                        TMAX.det + TMIN.det + PRCP.det + WESD.det, 
                      family = "binomial",
                      data = small.training.set)
stargazer(training.model,
          type = "html",
          dep.var.labels = "Cancelled",
          title = "New Multivariable Logistic Regression",
          single.row = TRUE)
```

```{r}
training.set$pred.glm <- stats::predict.glm(training.model,
                                        training.set,
                                        type = "response")

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.glm > 0.5, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            test.set)

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml)))$Result$estimate


test.set$pred.glm <- stats::predict.glm(training.model,
                                        test.set,
                                        type = "response")
conf.mat <- table(list(test.set$pred.glm > 0.1, test.set$CANCELLED))
rownames(conf.mat) <- c("Predicted No Cancellation", "Predicted Cancellation")

kable(conf.mat, 
      caption = "New Multivariable Logistic Regression Test Set Confusion Matrix", 
      align  = "l", 
      # row.names = TRUE,
      col.names = c("No Cancellation", "Cancellation"))

kable(data.frame(training.Kappa.val.ml, fmsb::Kappa.test(conf.mat)$Result$estimate),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "New Multivariable Logistic Regression Kappa Value")
```


## A New Random Forest Model

Similarly, we can make a new Random Forest model using the cutoff that we found to be optimal. HOW DO I CHANGE THE CUTOFFS???

```{r}
load("rf.model")
# CHANGE CUTOFF
kable(alex$results, caption = "Multivariable Random Forest Results")

# alex$finalModel
# Add this back

training.set$pred.ml <- predict(alex,
                                type = "prob",
                                training.set)[2]

training.Kappa.val.ml <- (fmsb::Kappa.test(table(training.set$pred.ml > 0.35, training.set$CANCELLED)))$Result$estimate

test.set$pred.ml <- predict(alex,
                            type = "prob",
                            test.set)[2]

test.Kappa.val.ml <- (fmsb::Kappa.test(table(test.set$CANCELLED, test.set$pred.ml > 0.35)))$Result$estimate

kable(data.frame(training.Kappa.val.ml, test.Kappa.val.ml),
      col.names = c("Training Kappa", "Test Kappa"),
      caption = "New Random Forest Kappa Values")
```
